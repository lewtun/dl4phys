{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4375c2f8-249a-4f6d-8089-2b647bd83ecb",
   "metadata": {},
   "source": [
    "# Lecture 3 - Neural network deep dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6c97c-1780-40a8-a739-f4c962182986",
   "metadata": {},
   "source": [
    "> A deep dive into optimising neural networks with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f2762-c6b8-4368-8042-336c40a448d8",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "* Understand what stochastic gradient descent is and how to minimise functions with it in PyTorch\n",
    "* Understand all the ingredients needed to define a `Learner` in fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfe094-686e-40e7-8eaa-0d4991a2243c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Chapter 4 of [_Deep Learning for Coders with fastai & PyTorch_](https://github.com/fastai/fastbook) by Jeremy Howard and Sylvain Gugger.\n",
    "* [What is `torch.nn` really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html#what-is-torch-nn-really) by Jeremy Howard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699fa5d4-0c13-4e79-821c-e2701c4279c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73261150-b566-48b0-812f-293e97564d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c70888-089f-42ec-904a-a271a66c5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Suppress logs to keep things tidy\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c5b5f-3b6c-4c99-a4aa-94393105d7a4",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5955a-ecce-4d19-8f2b-08581a0fdac6",
   "metadata": {},
   "source": [
    "In lecture 2, we focused on optimising simple functions with stochastic gradient descent. Let's now tackle a real-world problem using neural networks! We'll use the $N$-subjettiness dataset from lecture 1 that represents jets in terms of $\\tau_N^{(\\beta)}$ variables that measure the radiation about $N$ axes in the jet according to an angular exponent $\\beta>0$. As usual, we'll load the dataset from the Hugging Face Hub and convert it to a Pandas `DataFrame` via the `to_pandas()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67c6c7d-f7fc-44bf-8ba0-448d66495554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10402dcc15ee4c48a6fcf17c38cff073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT</th>\n",
       "      <th>mass</th>\n",
       "      <th>tau_1_0.5</th>\n",
       "      <th>tau_1_1</th>\n",
       "      <th>tau_1_2</th>\n",
       "      <th>tau_2_0.5</th>\n",
       "      <th>tau_2_1</th>\n",
       "      <th>tau_2_2</th>\n",
       "      <th>tau_3_0.5</th>\n",
       "      <th>tau_3_1</th>\n",
       "      <th>...</th>\n",
       "      <th>tau_4_0.5</th>\n",
       "      <th>tau_4_1</th>\n",
       "      <th>tau_4_2</th>\n",
       "      <th>tau_5_0.5</th>\n",
       "      <th>tau_5_1</th>\n",
       "      <th>tau_5_2</th>\n",
       "      <th>tau_6_0.5</th>\n",
       "      <th>tau_6_1</th>\n",
       "      <th>tau_6_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>543.633944</td>\n",
       "      <td>25.846792</td>\n",
       "      <td>0.165122</td>\n",
       "      <td>0.032661</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.048830</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024336</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>7.706005e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>452.411860</td>\n",
       "      <td>13.388679</td>\n",
       "      <td>0.162938</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.095902</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.079750</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.044211</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>6.175314e-05</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>3.670517e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429.495258</td>\n",
       "      <td>32.021091</td>\n",
       "      <td>0.244436</td>\n",
       "      <td>0.065901</td>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.155202</td>\n",
       "      <td>0.038807</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>0.025339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078205</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.052374</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>9.395772e-05</td>\n",
       "      <td>0.037572</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>2.237277e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512.675443</td>\n",
       "      <td>6.684734</td>\n",
       "      <td>0.102580</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.086306</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.068169</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.027895</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>4.400042e-06</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>6.731099e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>527.956859</td>\n",
       "      <td>133.985415</td>\n",
       "      <td>0.407009</td>\n",
       "      <td>0.191839</td>\n",
       "      <td>0.065169</td>\n",
       "      <td>0.291460</td>\n",
       "      <td>0.105479</td>\n",
       "      <td>0.029753</td>\n",
       "      <td>0.209341</td>\n",
       "      <td>0.049187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143768</td>\n",
       "      <td>0.033249</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.135407</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>2.593460e-03</td>\n",
       "      <td>0.110805</td>\n",
       "      <td>0.023179</td>\n",
       "      <td>2.202088e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pT        mass  tau_1_0.5   tau_1_1   tau_1_2  tau_2_0.5   tau_2_1  \\\n",
       "0  543.633944   25.846792   0.165122  0.032661  0.002262   0.048830  0.003711   \n",
       "1  452.411860   13.388679   0.162938  0.027598  0.000876   0.095902  0.015461   \n",
       "2  429.495258   32.021091   0.244436  0.065901  0.005557   0.155202  0.038807   \n",
       "3  512.675443    6.684734   0.102580  0.011369  0.000170   0.086306  0.007760   \n",
       "4  527.956859  133.985415   0.407009  0.191839  0.065169   0.291460  0.105479   \n",
       "\n",
       "    tau_2_2  tau_3_0.5   tau_3_1  ...  tau_4_0.5   tau_4_1   tau_4_2  \\\n",
       "0  0.000044   0.030994  0.001630  ...   0.024336  0.001115  0.000008   \n",
       "1  0.000506   0.079750  0.009733  ...   0.056854  0.005454  0.000072   \n",
       "2  0.002762   0.123285  0.025339  ...   0.078205  0.012678  0.000567   \n",
       "3  0.000071   0.068169  0.005386  ...   0.044705  0.002376  0.000008   \n",
       "4  0.029753   0.209341  0.049187  ...   0.143768  0.033249  0.003689   \n",
       "\n",
       "   tau_5_0.5   tau_5_1       tau_5_2  tau_6_0.5   tau_6_1       tau_6_2  label  \n",
       "0   0.004252  0.000234  7.706005e-07   0.000000  0.000000  0.000000e+00      0  \n",
       "1   0.044211  0.004430  6.175314e-05   0.037458  0.003396  3.670517e-05      0  \n",
       "2   0.052374  0.005935  9.395772e-05   0.037572  0.002932  2.237277e-05      0  \n",
       "3   0.027895  0.001364  4.400042e-06   0.009012  0.000379  6.731099e-07      0  \n",
       "4   0.135407  0.029054  2.593460e-03   0.110805  0.023179  2.202088e-03      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubjet_ds = load_dataset(\"dl4phys/top_tagging_nsubjettiness\")\n",
    "df = nsubjet_ds[\"train\"].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f61c3-7372-42a9-ba7e-12a28260fecf",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e87bc-05c4-4b26-8bdf-a7e8180b457b",
   "metadata": {},
   "source": [
    "In lecture 1, we used the `TabularDataLoaders.from_df()` method to quickly create dataloaders for the train and validation sets. In this lecture, we'll be working with PyTorch tensors directly, so we'll take a different approach. To get started, we'll need to split our data into a training and validation sets. We can do this quickly via the `train_test_split()` function from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e62de99-b6ed-477e-82a2-ba8f880d53ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((908250, 21), (302750, 21))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(df, random_state=42)\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cf9bf-e72c-4c27-9722-57f1272199eb",
   "metadata": {},
   "source": [
    "This has allocated 75% of our original dataset to `train_df` and the remainder to `valid_df`. Now that we have these `DataFrames`, the next thing we'll need are tensors for the features $(p_T, m, \\tau_1^{(0.5)}, \\tau_1^{(1)}, \\tau_1^{(2)}, \\ldots )$ and labels. There is, however, one potential problem: the jet $p_T$ and mass have much larger scales than the $N$-subjettiness $\\tau_N^{(\\beta)}$ features. We can see this by summarising the statistics of the training set with the `describe()` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "439362a9-115f-4250-8b2a-e226dab6a4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT</th>\n",
       "      <th>mass</th>\n",
       "      <th>tau_1_0.5</th>\n",
       "      <th>tau_1_1</th>\n",
       "      <th>tau_1_2</th>\n",
       "      <th>tau_2_0.5</th>\n",
       "      <th>tau_2_1</th>\n",
       "      <th>tau_2_2</th>\n",
       "      <th>tau_3_0.5</th>\n",
       "      <th>tau_3_1</th>\n",
       "      <th>...</th>\n",
       "      <th>tau_4_0.5</th>\n",
       "      <th>tau_4_1</th>\n",
       "      <th>tau_4_2</th>\n",
       "      <th>tau_5_0.5</th>\n",
       "      <th>tau_5_1</th>\n",
       "      <th>tau_5_2</th>\n",
       "      <th>tau_6_0.5</th>\n",
       "      <th>tau_6_1</th>\n",
       "      <th>tau_6_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "      <td>908250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>487.107393</td>\n",
       "      <td>88.090520</td>\n",
       "      <td>0.366716</td>\n",
       "      <td>0.198446</td>\n",
       "      <td>0.319559</td>\n",
       "      <td>0.222759</td>\n",
       "      <td>0.079243</td>\n",
       "      <td>0.072535</td>\n",
       "      <td>0.148137</td>\n",
       "      <td>0.035372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112024</td>\n",
       "      <td>0.022150</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.015329</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>0.070679</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.500366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>48.568267</td>\n",
       "      <td>48.393646</td>\n",
       "      <td>0.186922</td>\n",
       "      <td>0.339542</td>\n",
       "      <td>2.003898</td>\n",
       "      <td>0.110955</td>\n",
       "      <td>0.125155</td>\n",
       "      <td>0.674091</td>\n",
       "      <td>0.072627</td>\n",
       "      <td>0.051869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059393</td>\n",
       "      <td>0.032004</td>\n",
       "      <td>0.155468</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.022866</td>\n",
       "      <td>0.107641</td>\n",
       "      <td>0.046571</td>\n",
       "      <td>0.017133</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>225.490387</td>\n",
       "      <td>-0.433573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>452.879289</td>\n",
       "      <td>39.958178</td>\n",
       "      <td>0.224456</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>0.139269</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.094603</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069037</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.051012</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.036142</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>485.894050</td>\n",
       "      <td>99.887418</td>\n",
       "      <td>0.380172</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.045887</td>\n",
       "      <td>0.222763</td>\n",
       "      <td>0.061597</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.028501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110220</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.086045</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>520.506446</td>\n",
       "      <td>126.518545</td>\n",
       "      <td>0.477122</td>\n",
       "      <td>0.240550</td>\n",
       "      <td>0.074417</td>\n",
       "      <td>0.299708</td>\n",
       "      <td>0.108207</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.196156</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151137</td>\n",
       "      <td>0.029990</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.121905</td>\n",
       "      <td>0.021089</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.100437</td>\n",
       "      <td>0.015359</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>647.493145</td>\n",
       "      <td>299.211555</td>\n",
       "      <td>2.431888</td>\n",
       "      <td>6.013309</td>\n",
       "      <td>37.702422</td>\n",
       "      <td>2.218956</td>\n",
       "      <td>5.392683</td>\n",
       "      <td>33.352249</td>\n",
       "      <td>1.917912</td>\n",
       "      <td>4.502011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.616280</td>\n",
       "      <td>3.753716</td>\n",
       "      <td>21.161948</td>\n",
       "      <td>1.407356</td>\n",
       "      <td>3.158352</td>\n",
       "      <td>17.645603</td>\n",
       "      <td>1.388879</td>\n",
       "      <td>3.127371</td>\n",
       "      <td>17.340970</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pT           mass      tau_1_0.5        tau_1_1  \\\n",
       "count  908250.000000  908250.000000  908250.000000  908250.000000   \n",
       "mean      487.107393      88.090520       0.366716       0.198446   \n",
       "std        48.568267      48.393646       0.186922       0.339542   \n",
       "min       225.490387      -0.433573       0.000000       0.000000   \n",
       "25%       452.879289      39.958178       0.224456       0.058381   \n",
       "50%       485.894050      99.887418       0.380172       0.166016   \n",
       "75%       520.506446     126.518545       0.477122       0.240550   \n",
       "max       647.493145     299.211555       2.431888       6.013309   \n",
       "\n",
       "             tau_1_2      tau_2_0.5        tau_2_1        tau_2_2  \\\n",
       "count  908250.000000  908250.000000  908250.000000  908250.000000   \n",
       "mean        0.319559       0.222759       0.079243       0.072535   \n",
       "std         2.003898       0.110955       0.125155       0.674091   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.006443       0.139269       0.025638       0.001565   \n",
       "50%         0.045887       0.222763       0.061597       0.008788   \n",
       "75%         0.074417       0.299708       0.108207       0.022441   \n",
       "max        37.702422       2.218956       5.392683      33.352249   \n",
       "\n",
       "           tau_3_0.5        tau_3_1  ...      tau_4_0.5        tau_4_1  \\\n",
       "count  908250.000000  908250.000000  ...  908250.000000  908250.000000   \n",
       "mean        0.148137       0.035372  ...       0.112024       0.022150   \n",
       "std         0.072627       0.051869  ...       0.059393       0.032004   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.094603       0.013308  ...       0.069037       0.007949   \n",
       "50%         0.148810       0.028501  ...       0.110220       0.017609   \n",
       "75%         0.196156       0.046588  ...       0.151137       0.029990   \n",
       "max         1.917912       4.502011  ...       1.616280       3.753716   \n",
       "\n",
       "             tau_4_2      tau_5_0.5        tau_5_1        tau_5_2  \\\n",
       "count  908250.000000  908250.000000  908250.000000  908250.000000   \n",
       "mean        0.008670       0.088400       0.015329       0.004875   \n",
       "std         0.155468       0.051949       0.022866       0.107641   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000188       0.051012       0.004936       0.000079   \n",
       "50%         0.000787       0.086045       0.011755       0.000387   \n",
       "75%         0.002006       0.121905       0.021089       0.001103   \n",
       "max        21.161948       1.407356       3.158352      17.645603   \n",
       "\n",
       "           tau_6_0.5        tau_6_1        tau_6_2          label  \n",
       "count  908250.000000  908250.000000  908250.000000  908250.000000  \n",
       "mean        0.070679       0.011019       0.002914       0.500366  \n",
       "std         0.046571       0.017133       0.078247       0.500000  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.036142       0.002977       0.000033       0.000000  \n",
       "50%         0.067797       0.008028       0.000193       1.000000  \n",
       "75%         0.100437       0.015359       0.000635       1.000000  \n",
       "max         1.388879       3.127371      17.340970       1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710585ed-85ec-4684-98ef-b740c93e2687",
   "metadata": {},
   "source": [
    "As we saw in lecture 2, SGD can struggle to optimise the loss function when the feature scales are very different. To handle this, it is common to _normalize_ the features in some way. One way to do this is by rescaling all the features $x_i$ to lie in the interval $[0,1]$:\n",
    "\n",
    "$$ x_i' = \\frac{x_i - x_{i,\\mathrm{min}}}{x_{i,\\mathrm{max}} - x_{i,\\mathrm{min}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02585396-0a97-40a0-8461-a1921e6461f1",
   "metadata": {},
   "source": [
    "To apply this normalization, let's first grab the NumPy arrays of the features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2745a555-ffa3-4528-836e-3934d9293f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice out all feature columns\n",
    "train_x = train_df.iloc[:, :-1].values\n",
    "# Slice out the label column\n",
    "train_y = train_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dd6fe-cd29-4a67-906f-3aaaa90d2289",
   "metadata": {},
   "source": [
    "Next, we use the `MinMaxScaler` from scikit-learn to apply the normalization on the features array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ad9afff-ba7a-44af-8e6c-678574411ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "# Sanity check the normalization worked\n",
    "np.min(train_x), np.max(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec2e54-0a7f-4abc-944f-29a403aeee21",
   "metadata": {},
   "source": [
    "Great, this worked! Now that our features are all nicely normalised, let's convert these NumPy arrays to PyTorch tensors. PyTorch provides a handy `from_numpy()` method that allows us to do the conversion easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a69d9dfe-246e-4087-99ef-864480d3bc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([908250, 20]), torch.Size([908250]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast to float32\n",
    "train_x = torch.from_numpy(train_x).float()\n",
    "train_y = torch.from_numpy(train_df.iloc[:, -1].values)\n",
    "# Sanity check on the shapes\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056e46a-9386-4652-8996-d5ce37adfad1",
   "metadata": {},
   "source": [
    "Okay, now that we have our tensors it's time to train a neural network on the features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cce97-501f-46f9-b7cb-3641aac5b572",
   "metadata": {},
   "source": [
    "## Logistic regression as a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "091368ff-3b2a-44db-a656-eb1a73162057",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "weights = torch.randn(20, 2) / math.sqrt(20)\n",
    "weights.requires_grad_()\n",
    "bias = torch.ones(2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1c342c4-f73f-4eb9-8191-630d2ec847c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38fbb26d-2247-4284-b34a-c84359aef066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5103, -0.9171], grad_fn=<SelectBackward0>), torch.Size([1024, 2]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 1024  # batch size\n",
    "\n",
    "xb = train_x[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "771c575a-9cb9-4013-a9e7-1f1ced1c892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(predictions, target):\n",
    "    return -predictions[range(target.shape[0]), target].mean()\n",
    "\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a5f66f4-fdd1-4de3-a730-1ab9aded0859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7619, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "yb = train_y[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02d93a12-2d60-4583-ad65-b2fbeb1b753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86b666f6-b1ce-466b-bc2d-940f2b5619ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5020)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e60d196-bfb9-47f3-af60-187661015f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e36ac8f0a94427a94eda0418b05f9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "num_epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-2  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "n = len(train_df)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"num_epochs\"):\n",
    "    for i in tqdm(range((n - 1) // bs + 1), leave=False):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = train_x[start_i:end_i]\n",
    "        yb = train_y[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b802fafb-28f2-40f5-99cf-27e8ff3668c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6026, grad_fn=<NegBackward0>) tensor(0.8519)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeec624-ef8c-4d9e-a037-c9d30f9f1530",
   "metadata": {},
   "source": [
    "## Refactor using torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77b6673c-2e63-4e0e-8cf4-bd4f369f0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "06f0cd05-4437-48b5-ad9f-88f1364ab932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5866, grad_fn=<NllLossBackward0>) tensor(0.8570)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10569943-f405-4ecf-902c-f5c1acbec110",
   "metadata": {},
   "source": [
    "## Refactor using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "654d85e5-3c22-47a7-87a6-c032f9fa7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(20, 2) / math.sqrt(20))\n",
    "        self.bias = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4141d897-2802-47a5-aa95-5192c981b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6916, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressor()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d766653c-6063-4189-bad2-4a0fa2a9ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = train_x[start_i:end_i]\n",
    "            yb = train_y[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8014f42b-b5b7-4b1e-9ee6-652e14f5aa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5771, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1caa4a51-bd25-416c-9e7b-58489237abbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8560)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6be15c-db30-4054-bea8-468549e2b543",
   "metadata": {},
   "source": [
    "### Refactor using optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01fc0d1c-79fe-4fa5-a668-f7c386c527d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3bfa7241-b920-4c65-ae62-51e1b92efb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7049, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5792, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = LogisticRegressor()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = train_x[start_i:end_i]\n",
    "        yb = train_y[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae9c6e-3e5d-4fa0-89a4-6dbd7c623280",
   "metadata": {},
   "source": [
    "## Refactor using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "26fecc2d-1d2a-4d81-9e14-2627e6f27e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f64c224e-420e-4fd5-8509-b6cc49d0dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e22a0c5-c9e1-48e7-b60b-0dd5424ce562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5940, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs : i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89081f3-236c-491c-bc51-d418aa34649e",
   "metadata": {},
   "source": [
    "### Refactor using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3d6e7b34-4d12-46a2-9de9-9fff2d48f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09623063-19b2-4c06-9ed2-585231bcb885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5713, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c092c9-30fa-4afa-ba3f-88d46c16a3ff",
   "metadata": {},
   "source": [
    "### Add validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f11cced-6eaf-4795-8524-1b82939915ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "# Slice out all feature columns and cast to float32\n",
    "valid_x = torch.from_numpy(scaler.fit_transform(valid_df.iloc[:, :-1].values)).float()\n",
    "# Slice out the label column and convert to matrix of shape (num_examples, 1)\n",
    "valid_y = torch.from_numpy(valid_df.iloc[:, -1].values)\n",
    "valid_ds = TensorDataset(valid_x, valid_y)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a601b9e-6d82-49a4-b829-1ec069f1aa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6379)\n",
      "1 tensor(0.5772)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "191324a4-1ff1-4b9b-915d-b30deb82a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    acc = accuracy(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89d97a3f-f4a8-4349-a892-16578680702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in tqdm(train_dl):\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums, accs = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84c7ed9d-c969-457e-88f1-52f9b25d9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0a8cefe-bfbe-4b5d-9977-4d4ee6f8803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed03b023a243a9ac5f6a1da8245304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6251985956793674 0.8593955408599532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621d6902fc614e5e89d8216128218734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5675691612205852 0.8616317094732768\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c8434-9bab-40d0-b7d5-e9853cee1762",
   "metadata": {},
   "source": [
    "## Going deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fb24009b-a8a2-4d75-91c8-786c83cf68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91e0dca1c6542369db360061070765d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6895127527026673 0.8562180017491788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44059ff83b74402b43a66df37837c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.676258602224235 0.8750156895994254\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(20, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 2),\n",
    ")\n",
    "lr = 1e-3\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f023-cb2c-442f-9d01-84e961b12ac4",
   "metadata": {},
   "source": [
    "## Fasti style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f8c1cce0-2c97-4de7-bd3c-829d3c28de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "100debb8-f1b6-453e-8d2f-138b1699a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func, opt_func=SGD, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "99e8b32b-da01-454d-a4dd-4912c207c9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.010964781977236271)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a446a63a-082c-4fef-bcd6-b36c075f15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.611372</td>\n",
       "      <td>0.597583</td>\n",
       "      <td>0.869870</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478586</td>\n",
       "      <td>0.475628</td>\n",
       "      <td>0.870259</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250f65a-222e-409b-aa97-2ebc862f39f7",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Use `nn.Module` to create the NN and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1130f9-ed4b-46a7-b546-b46470dffb86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
